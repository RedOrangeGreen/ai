<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ollama AI, tinyllama, C++ Interfacing, Ubuntu</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            font-size: 16px;
            line-height: 1.5;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f9f9f9;
        }

        h1, h2 {
            color: #333;
            font-weight: bold;
        }

        h1 {
            font-size: 28px;
            text-align: center;
            margin-bottom: 30px;
        }

        h2 {
            font-size: 20px;
            margin-top: 30px;
            margin-bottom: 10px;
        }

        .command-list {
            list-style-type: none;
            padding: 0;
            margin: 0;
        }

        .command-item {
            margin-bottom: 10px;
            padding: 10px;
            background-color: #fff;
            border: 1px solid #ddd;
            border-radius: 5px;
            font-family: monospace;
            user-select: all;
            cursor: text;
        }

        a {
            color: #007BFF;
            text-decoration: none;
        }

        a:hover {
            text-decoration: underline;
        }
    </style>
</head>
<body>
    <h1>Ollama AI, tinyllama, C++ Interfacing, Ubuntu</h1>
    <p>
        Ubuntu commands to use the tinyllama model with Ollama AI and interface to it from C++. Tinyllama is a small and fast model, even when using just a CPU for inference. These commands assume the Docker version of Ollama is installed and running. If not, see <a href="https://redorangegreen.github.io/ai/ollama/quickstart" target="_blank" rel="noopener noreferrer">here</a> for instructions.
    </p>
    <p>
        For reference, here's the full <a href="https://ollama.com/library" target="_blank" rel="noopener noreferrer">Ollama AI Models Library</a>.
    </p>
    <ul class="command-list">
        <li class="command-item">sudo docker exec -it aidoings ollama run tinyllama</li>
        <li class="command-item">What is a mouse?</li>
        <li class="command-item">/bye</li>
        <li class="command-item"><a href="http://localhost:11434" target="_blank" rel="noopener noreferrer">http://localhost:11434</a></li>        
        <li class="command-item">sudo apt install curl</li>
        <li class="command-item">sudo apt install libcurl4-openssl-dev</li>
        <li class="command-item">
            curl -X POST http://localhost:11434/api/generate -d '{
            "model": "tinyllama",
            "prompt": "Why is the sky blue?"
            }'
        </li>
        <li class="command-item"><a href="https://www.perplexity.ai" target="_blank" rel="noopener noreferrer">Open Perplexity AI</a></li>
        <li class="command-item">Show me how to use Ollama which is listening on port 11434 from C++; use model "tinyllama" (this exact name, nothing before or after) and libcurl</li>
        <li class="command-item">The response from Ollama contains lines like the following; Without using a third party library enhance the code to extract the second string from each third field from such lines, without adding extra spaces add each extracted string to a string called 'result', as each string is added display some indication then output 'result' at the end... {"model":"tinyllama","created_at":"2024-08-10T10:02:19.79692779Z","response":"The","done":false}</li>
        <li class="command-item">vi useai.cpp</li>
        <li class="command-item">g++ useai.cpp -o useai -lcurl</li>
        <li class="command-item">./useai</li>
    </ul>
</body>
</html>
